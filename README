diagram of this project:
https://prnt.sc/HUIljZAWzggT
just me an open source(vagrant niye video)

#some common command every debian based server:
apt update -y
apt install net-tools -y
apt install tcpdump -y
apt install vim -y
apt install iputils-ping -y


kubectl create -f service1-rc.yaml
kubectl create -f service-svc.yaml
kubectl get pods
#List All Services in Default Namespace:
kubectl get svc
kubectl get svc server1-service
kubectl get ep
kubectl get pods -o wide
#get node ip/vm ip
kubectl get nnode -o wide
kubectl delete pod <pod name>
docker build -t <dockerhub account id name>/<image name> .
docker push <dockerhub account id name>/<image name>
kubectl exec -it loadbalancer-rc-9d272 /bin/bash
kubectl get svc -n default (run this locally or host machine)
curl http://server1-service:80
#for testing purpose use port forwading
kubectl port-forward svc/server1-service 8080:80
#for external use of service we use nodePort service
nodePort: 30002 
kubectl get svc server1-service -o yaml
kubectl describe svc <service-name>
#getting hit module, working internal port not working NotePort when tcpdump because port comming after forward 30001--->80
tcpdump -i any port 80 -vv
#check pod logs
kubectl get logs <pod name>




#Why this set up ?
#Clarification of Workflow:
1. Pod Communication:

Pods communicate with each other or with external resources via Services. A Pod doesn't need to know the IP 
addresses of other Pods directly; it interacts with them through a Service, which abstracts away the complexity.

2. Service Communication:

When a request is directed at a Service, Kubernetes routes the request to one of the Pods that match the Service’s label selector. 
This selection process includes built-in load balancing, where requests are distributed among all available Pods.

3. External Load Balancing:

If your application needs to handle external traffic, a load balancer (such as Nginx in your Docker setup) can be configured to route 
incoming traffic to various Kubernetes Services. These Services, in turn, route traffic to the appropriate Pods. 
This setup ensures that external users can reach your application while maintaining a balanced load distribution across your application’s 
instances.

Example Workflow:
1. External User Request: An external user sends a request to your application’s URL.
2. Nginx Load Balancer: The request first hits the Nginx load balancer, which is configured to route requests to one of the Kubernetes Services.
3. Service Routing: The Service then forwards the request to one of the Pods it manages, ensuring that the traffic is balanced across 
multiple instances of the application.
4. Pod Handling: The selected Pod processes the request and returns the response, which is then routed back through the Service, 
load balancer, and finally to the external user.